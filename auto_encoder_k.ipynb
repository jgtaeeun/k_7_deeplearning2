{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43bfe56f-dc93-47bb-849a-4470f24a0676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581348b3-08e4-4bfd-a60f-817f45678b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "my_transform = v2.Compose([\n",
    "    #v2.ToTensor()\n",
    "    v2.ToImage(), \n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "dataset = MNIST('data', download=True, transform=my_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9095c5-1478-4dba-9e41-3e4076e07ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "for X_train, y_label in data_loader:\n",
    "    print(X_train.shape, y_label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ae59d10-ef25-4607-a258-11560b27a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding='same'), # 28\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding='same'), # 14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding='same'), # 7\n",
    "            #nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2),\n",
    "            #nn.Conv2d(128, 256, 3, padding='same'),\n",
    "            #nn.BatchNorm2d(256),\n",
    "            #nn.ReLU(),\n",
    "            #nn.MaxPool2d(2),\n",
    "            #nn.Conv2d(256, 512, 3, padding='same'),\n",
    "            #nn.BatchNorm2d(512),\n",
    "            #nn.ReLU(),\n",
    "            #nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.encoder_fc = nn.Sequential(\n",
    "            nn.Linear(128*7*7, encoder_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder_cnn(x)\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.encoder_fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0f2b1fdb-6d63-4998-ab2f-98a647eab0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image = torch.rand(1, 1, 224, 224)\n",
    "image = torch.rand(1, 1, 28, 28)\n",
    "encoder = Encoder(128)\n",
    "encoder(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "54db4532-8907-400d-9dce-01fd2b7aa263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoder_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_input = nn.Sequential(\n",
    "            nn.Linear(encoder_dim, 128*7*7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder_cnn = nn.Sequential(\n",
    "            #nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1),\n",
    "            #nn.BatchNorm2d(256),\n",
    "            #nn.ReLU(),\n",
    "            #nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            #nn.BatchNorm2d(128),\n",
    "            #nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1), # 14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1), # 28\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            #nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            #nn.ReLU(),            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_input(x)\n",
    "        x = torch.unflatten(x, 1, (128, 7, 7))\n",
    "        #print(x.shape)\n",
    "        x = self.decoder_cnn(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2235a281-9585-445d-a5c1-e488190926ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 28, 28])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_image = torch.rand(1, 128)\n",
    "decoder = Decoder(128)\n",
    "decoder (encoded_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "946fdff6-5000-4c7a-bdcf-9996235650b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for X_train, y_label in data_loader:\n",
    "    print(X_train.shape, y_label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4cd0e0d7-87bb-491b-899f-8a09c82d8fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\torch-book\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([32, 1, 28, 28])) that is different to the input size (torch.Size([32, 32, 28, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam([{'params': encoder.parameters()},\n",
    "                        {'params': decoder.parameters()}],\n",
    "                        lr=0.0001)\n",
    "loss_fn = nn.MSELoss()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "for X_train, y_label in data_loader:\n",
    "    optimizer.zero_grad()\n",
    "    #print(X_train.shape)\n",
    "    images = X_train.to(device)\n",
    "    encoded_outputs = encoder(images)\n",
    "    decoded_outputs = decoder(encoded_outputs)\n",
    "    loss = loss_fn(decoded_outputs, images)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac4f707-76c3-4078-bec9-532f3b3236de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = MNIST('data', download=True, train=True, transform=my_transform)\n",
    "dataset_test = MNIST('data', download=True, train=False, transform=my_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6baa0f14-5e5c-41ce-b598-a60497015679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as v2\n",
    "\n",
    "def make_dataset_noisy(train, noise_factor=0.2):\n",
    "    dataset = MNIST('data', download=True, train=train, transform=my_transform)\n",
    "\n",
    "    noise = noise_factor * torch.randint(0, 256, dataset.data.shape, dtype=torch.float32)\n",
    "    noisy_data = dataset.data.float() + noise\n",
    "    noisy_data = torch.clamp(noisy_data, 0.0, 255.0).type(torch.uint8)\n",
    "    dataset.data = noisy_data\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37f45813-e5a9-46e6-98e9-76fa71645de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), torch.Size([1, 28, 28]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_noisy_train = make_dataset_noisy(train=True)\n",
    "dataset_train[0][0].shape, dataset_noisy_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fb3f6df-64df-4446-9030-bb033f0be862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def visualize_mnist(img_origin, img_noisy):\n",
    "#     for i in range(4):\n",
    "#       print(img_origin[i][0].shape, img_noisy[i][0].shape)\n",
    "#       plt.subplot(2, 4, i+1)\n",
    "#       plt.imshow(img_origin[i][0].squeeze())\n",
    "#       plt.subplot(2, 4, i+5)\n",
    "#       plt.imshow(img_noisy[i][0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "888d48f3-b798-48d2-b328-6d9f0c61fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_mnist(dataset_train, dataset_noisy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f278354-f2c9-415a-8ab4-dba1312ef442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1aad2e33650>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(dataset_train[0][0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f994b11-79bf-4b23-92e3-4485a9ae3cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
