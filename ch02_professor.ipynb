{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd11560f-0e24-495d-923a-512fc736a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372a75f-729a-4d8f-8b4e-5d5203cb9d11",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aea30d8-3126-4d90-adbb-c1c7a82623d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    dataset = pd.read_csv(\"data/car_evaluation.csv\")\n",
    "    categorical_columns = [\"price\", \"maint\", \"doors\", \"persons\", \"lug_capacity\", \"safety\"]\n",
    "    for category in categorical_columns:\n",
    "        dataset[category] = dataset[category].astype(\"category\")\n",
    "\n",
    "    price = dataset[\"price\"].cat.codes.values\n",
    "    maint = dataset[\"maint\"].cat.codes.values\n",
    "    doors = dataset[\"doors\"].cat.codes.values\n",
    "    persons = dataset[\"persons\"].cat.codes.values\n",
    "    lug_capacity = dataset[\"lug_capacity\"].cat.codes.values\n",
    "    safety = dataset[\"safety\"].cat.codes.values\n",
    "    \n",
    "    categorical_data =  np.stack([price, maint, doors, persons, lug_capacity, safety],1)\n",
    "    categorical_data = torch.tensor(categorical_data, dtype=torch.int64)\n",
    "\n",
    "    categorical_colum_sizes = [len(dataset[column].cat.categories) for column in categorical_columns]\n",
    "    categorical_embedding_sizes = [(col_size, min(50, (col_size)+1) // 2) for col_size in categorical_colum_sizes]\n",
    "    \n",
    "    outputs = dataset[\"output\"].astype(\"category\").cat.codes.values\n",
    "    outputs = torch.tensor(outputs, dtype=torch.int64).flatten()\n",
    "    return  categorical_data, outputs,categorical_embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c9a486-c780-4b60-b16d-41a6ab422c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data, outputs,categorical_embedding_sizes =preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082fe4d8-a007-479b-a5a2-150278f037ee",
   "metadata": {},
   "source": [
    "## 데이터 훈련,테스트 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a6f9c7-616a-4ebf-b4b6-dc61476bb5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( categorical_data, outputs,test_size=0.2,  random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca94618-6523-4f77-a3b6-cd52dab4ddfd",
   "metadata": {},
   "source": [
    "## dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c61fce6c-1feb-4850-8cc8-41be6f788d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarEvaluationDataset(Dataset):\n",
    "    def __init__(self, categorical_data, outputs):\n",
    "        self.categorical_data=categorical_data\n",
    "        self.outputs=outputs\n",
    "    def __len__(self):\n",
    "        return len(self.categorical_data)\n",
    "    def __getitem__(self, idx):\n",
    "        X=self.categorical_data[idx]\n",
    "        y=self.outputs[idx]\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6501a2e1-9874-4fe1-8a44-72aa447177fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= CarEvaluationDataset(X_train, y_train)\n",
    "test_dataset=CarEvaluationDataset(X_test, y_test)\n",
    "train_loader=DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader=DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f815c7-cc09-4266-812b-d51ae9580891",
   "metadata": {},
   "source": [
    "## 모델, optimizer, loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b7e1b8-226c-4208-b8ff-29d2e0be3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_size, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        all_layers = []\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x_categorical):\n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = self.layers(x)\n",
    "        return x          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c2aeb73-6820-4d74-9848-bfdd6c17708a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (all_embeddings): ModuleList(\n",
       "    (0-2): 3 x Embedding(4, 2)\n",
       "    (3-5): 3 x Embedding(3, 2)\n",
       "  )\n",
       "  (embedding_dropout): Dropout(p=0.4, inplace=False)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=200, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.4, inplace=False)\n",
       "    (12): Linear(in_features=50, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(categorical_embedding_sizes, 4, [200, 100, 50], p=0.4)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4f95a0d-9206-401c-9156-4b1019860d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7583eebe-edc2-4f5b-b827-656943c7b8bb",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "238b5211-778e-4efd-bd86-996e95c5bd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.641559898853302\n",
      "0.6264883875846863\n",
      "0.5969135761260986\n",
      "0.5080094337463379\n",
      "0.5432601571083069\n",
      "0.654242992401123\n",
      "0.42240089178085327\n",
      "0.6040340065956116\n",
      "0.46807193756103516\n",
      "0.6929337382316589\n",
      "0.5118901133537292\n",
      "0.6174202561378479\n",
      "0.7030670642852783\n",
      "0.5443251132965088\n",
      "0.5464232563972473\n",
      "0.5439098477363586\n",
      "0.6161820888519287\n",
      "0.6234215497970581\n",
      "0.41722631454467773\n",
      "0.6559208631515503\n",
      "0.538278341293335\n",
      "0.5269096493721008\n",
      "0.5391817688941956\n",
      "0.638178825378418\n",
      "0.5419896245002747\n",
      "0.5316651463508606\n",
      "0.4670487940311432\n",
      "0.5842446088790894\n",
      "0.46199944615364075\n",
      "0.5388999581336975\n",
      "0.6017221212387085\n",
      "0.5442852973937988\n",
      "0.3886118531227112\n",
      "0.6581835150718689\n",
      "0.43312835693359375\n",
      "0.49796199798583984\n",
      "0.5686774849891663\n",
      "0.5693812966346741\n",
      "0.6117271184921265\n",
      "0.4214401841163635\n",
      "0.4332050383090973\n",
      "0.4922582805156708\n",
      "0.4540562629699707\n",
      "0.6923378109931946\n",
      "0.5036026239395142\n",
      "0.4557062089443207\n",
      "0.3417893648147583\n",
      "0.41791531443595886\n",
      "0.4774973690509796\n",
      "0.5376791954040527\n",
      "0.4725476801395416\n",
      "0.5757436156272888\n",
      "0.5554529428482056\n",
      "0.3766031861305237\n",
      "0.38262319564819336\n",
      "0.5638831257820129\n",
      "0.5009168982505798\n",
      "0.5294122099876404\n",
      "0.4426560401916504\n",
      "0.44284912943840027\n",
      "0.4088353216648102\n",
      "0.4209042489528656\n",
      "0.5600596070289612\n",
      "0.3908531665802002\n",
      "0.6043948531150818\n",
      "0.5311548113822937\n",
      "0.392085462808609\n",
      "0.3860209584236145\n",
      "0.4121682941913605\n",
      "0.46419548988342285\n",
      "0.7174909114837646\n",
      "0.3485207259654999\n",
      "0.4255882501602173\n",
      "0.6793885231018066\n",
      "0.44781550765037537\n",
      "0.524739146232605\n",
      "0.3492213487625122\n",
      "0.3866615891456604\n",
      "0.5031330585479736\n",
      "0.3244369924068451\n",
      "0.406616747379303\n",
      "0.4163144826889038\n",
      "0.3979162573814392\n",
      "0.49019357562065125\n",
      "0.6420189738273621\n",
      "0.409932017326355\n",
      "0.43953320384025574\n",
      "0.4266837239265442\n",
      "0.47005146741867065\n",
      "0.4447726309299469\n",
      "0.35963284969329834\n",
      "0.40985924005508423\n",
      "0.3191635012626648\n",
      "0.3057388961315155\n",
      "0.5771700739860535\n",
      "0.4436609148979187\n",
      "0.5804571509361267\n",
      "0.4967260956764221\n",
      "0.3631453514099121\n",
      "0.4761703908443451\n",
      "0.3455026149749756\n",
      "0.30785199999809265\n",
      "0.3088955879211426\n",
      "0.29655396938323975\n",
      "0.4800349175930023\n",
      "0.4268397092819214\n",
      "0.34566470980644226\n",
      "0.4757387638092041\n",
      "0.40970784425735474\n",
      "0.5198433995246887\n",
      "0.33164894580841064\n",
      "0.30330222845077515\n",
      "0.3471386134624481\n",
      "0.48957881331443787\n",
      "0.4083701968193054\n",
      "0.3958466649055481\n",
      "0.42526334524154663\n",
      "0.3956701159477234\n",
      "0.48556849360466003\n",
      "0.3680151104927063\n",
      "0.3507947623729706\n",
      "0.43074163794517517\n",
      "0.40024226903915405\n",
      "0.40393590927124023\n",
      "0.44584956765174866\n",
      "0.5153505206108093\n",
      "0.3969976007938385\n",
      "0.4252672493457794\n",
      "0.5733567476272583\n",
      "0.3738081753253937\n",
      "0.45903846621513367\n",
      "0.4545990228652954\n",
      "0.4559956192970276\n",
      "0.3251127302646637\n",
      "0.3734809458255768\n",
      "0.3380091190338135\n",
      "0.36326515674591064\n",
      "0.6284652948379517\n",
      "0.26262983679771423\n",
      "0.39568960666656494\n",
      "0.48195594549179077\n",
      "0.37237995862960815\n",
      "0.45745059847831726\n",
      "0.441990464925766\n",
      "0.7382708787918091\n",
      "0.30201685428619385\n",
      "0.406656950712204\n",
      "0.37388765811920166\n",
      "0.38805854320526123\n",
      "0.41817715764045715\n",
      "0.42527318000793457\n",
      "0.450158029794693\n",
      "0.2745307385921478\n",
      "0.34013602137565613\n",
      "0.35485273599624634\n",
      "0.3789537250995636\n",
      "0.3535008728504181\n",
      "0.3240000903606415\n",
      "0.4984527826309204\n",
      "0.48417383432388306\n",
      "0.3408452868461609\n",
      "0.43556684255599976\n",
      "0.4208478629589081\n",
      "0.2995777130126953\n",
      "0.2941531836986542\n",
      "0.33113372325897217\n",
      "0.5837200880050659\n",
      "0.40917110443115234\n",
      "0.3316086232662201\n",
      "0.3770519495010376\n",
      "0.4986806809902191\n",
      "0.30580711364746094\n",
      "0.2591540217399597\n",
      "0.45561498403549194\n",
      "0.42642930150032043\n",
      "0.551600456237793\n",
      "0.2647935152053833\n",
      "0.35176053643226624\n",
      "0.35774803161621094\n",
      "0.3940832316875458\n",
      "0.3626518249511719\n",
      "0.462617427110672\n",
      "0.4794997274875641\n",
      "0.29767006635665894\n",
      "0.5129043459892273\n",
      "0.37705469131469727\n",
      "0.3169146776199341\n",
      "0.32566556334495544\n",
      "0.46917274594306946\n",
      "0.3168891370296478\n",
      "0.5054037570953369\n",
      "0.3737563490867615\n",
      "0.38713034987449646\n",
      "0.4302806258201599\n",
      "0.41889339685440063\n",
      "0.5554429292678833\n",
      "0.3262842297554016\n",
      "0.428362637758255\n",
      "0.4149440824985504\n",
      "0.4458105266094208\n",
      "0.5151872634887695\n",
      "0.33996573090553284\n",
      "0.3347632884979248\n",
      "0.37243136763572693\n",
      "0.3637695014476776\n",
      "0.3257659673690796\n",
      "0.46811872720718384\n",
      "0.4507923424243927\n",
      "0.2853401005268097\n",
      "0.43704894185066223\n",
      "0.42780765891075134\n",
      "0.35115593671798706\n",
      "0.3095173239707947\n",
      "0.374287873506546\n",
      "0.33339762687683105\n",
      "0.4268966317176819\n",
      "0.46569564938545227\n",
      "0.37052398920059204\n",
      "0.556381106376648\n",
      "0.3566073477268219\n",
      "0.4057207405567169\n",
      "0.27567625045776367\n",
      "0.305335134267807\n",
      "0.32676640152931213\n",
      "0.4278263747692108\n",
      "0.3413540720939636\n",
      "0.41729632019996643\n",
      "0.3921997845172882\n",
      "0.47440293431282043\n",
      "0.32155781984329224\n",
      "0.4222186803817749\n",
      "0.28810247778892517\n",
      "0.37763264775276184\n",
      "0.5126024484634399\n",
      "0.4021834135055542\n",
      "0.2682781219482422\n",
      "0.5204612612724304\n",
      "0.2840484082698822\n",
      "0.34055259823799133\n",
      "0.38366076350212097\n",
      "0.47898441553115845\n",
      "0.3714011013507843\n",
      "0.41947004199028015\n",
      "0.33453696966171265\n",
      "0.3891376256942749\n",
      "0.3307272493839264\n",
      "0.5495469570159912\n",
      "0.2019965946674347\n",
      "0.36397919058799744\n",
      "0.23109115660190582\n",
      "0.6550559401512146\n",
      "0.29135632514953613\n",
      "0.3217473328113556\n",
      "0.31032174825668335\n",
      "0.4290156662464142\n",
      "0.41329482197761536\n",
      "0.4100857973098755\n",
      "0.2961910665035248\n",
      "0.2820129990577698\n",
      "0.22508090734481812\n",
      "0.3586667776107788\n",
      "0.4963622987270355\n",
      "0.4327555298805237\n",
      "0.17730943858623505\n",
      "0.2996123433113098\n",
      "0.4982837736606598\n",
      "0.5694912672042847\n",
      "0.3066253364086151\n",
      "0.2834838032722473\n",
      "0.4406318962574005\n",
      "0.28811606764793396\n",
      "0.29836463928222656\n",
      "0.3213581144809723\n",
      "0.4347684383392334\n",
      "0.3397359848022461\n",
      "0.3529691696166992\n",
      "0.34446629881858826\n",
      "0.35466474294662476\n",
      "0.44244715571403503\n",
      "0.3757779002189636\n",
      "0.3567710816860199\n",
      "0.5810871720314026\n",
      "0.3077167868614197\n",
      "0.2733585238456726\n",
      "0.3099031448364258\n",
      "0.5052222013473511\n",
      "0.4209566116333008\n",
      "0.3444613516330719\n",
      "0.38472363352775574\n",
      "0.3723958730697632\n",
      "0.33324772119522095\n",
      "0.48178285360336304\n",
      "0.40000373125076294\n",
      "0.30281975865364075\n",
      "0.3015609085559845\n",
      "0.3229290544986725\n",
      "0.3294832706451416\n",
      "0.3539413809776306\n",
      "0.24987725913524628\n",
      "0.44724443554878235\n",
      "0.3750683069229126\n",
      "0.34648165106773376\n",
      "0.3843749761581421\n",
      "0.2855323553085327\n",
      "0.3154020309448242\n",
      "0.2864666283130646\n",
      "0.257478803396225\n",
      "0.20787687599658966\n",
      "0.31441208720207214\n",
      "0.3999807834625244\n",
      "0.2932416498661041\n",
      "0.5436025261878967\n",
      "0.24025744199752808\n",
      "0.3477359712123871\n",
      "0.4023185670375824\n",
      "0.26576149463653564\n",
      "0.3915962874889374\n",
      "0.4638292193412781\n",
      "0.2374783605337143\n",
      "0.3626546561717987\n",
      "0.4287429749965668\n",
      "0.36053594946861267\n",
      "0.406686395406723\n",
      "0.27779915928840637\n",
      "0.36722490191459656\n",
      "0.28891026973724365\n",
      "0.3685516119003296\n",
      "0.30235329270362854\n",
      "0.3916506767272949\n",
      "0.3056739568710327\n",
      "0.41537073254585266\n",
      "0.3767942488193512\n",
      "0.32118290662765503\n",
      "0.2969115972518921\n",
      "0.20319756865501404\n",
      "0.3380192518234253\n",
      "0.3679107129573822\n",
      "0.25731587409973145\n",
      "0.4755900502204895\n",
      "0.30834394693374634\n",
      "0.4268367886543274\n",
      "0.409742146730423\n",
      "0.35290759801864624\n",
      "0.3071337342262268\n",
      "0.40886715054512024\n",
      "0.26497694849967957\n",
      "0.4603192210197449\n",
      "0.40353161096572876\n",
      "0.3073953092098236\n",
      "0.30245450139045715\n",
      "0.3087351322174072\n",
      "0.4146508276462555\n",
      "0.35605472326278687\n",
      "0.32468461990356445\n",
      "0.266573429107666\n",
      "0.37624186277389526\n",
      "0.31855636835098267\n",
      "0.4235759973526001\n",
      "0.30836132168769836\n",
      "0.43677857518196106\n",
      "0.3578017055988312\n",
      "0.3185272514820099\n",
      "0.22456271946430206\n",
      "0.3548791706562042\n",
      "0.3968658745288849\n",
      "0.29782068729400635\n",
      "0.5532553195953369\n",
      "0.3674316704273224\n",
      "0.34315937757492065\n",
      "0.3298284411430359\n",
      "0.46827760338783264\n",
      "0.3195818364620209\n",
      "0.24008554220199585\n",
      "0.4044678211212158\n",
      "0.39876240491867065\n",
      "0.4575617015361786\n",
      "0.39103126525878906\n",
      "0.276669979095459\n",
      "0.19449211657047272\n",
      "0.24547605216503143\n",
      "0.5207709074020386\n",
      "0.22883066534996033\n",
      "0.44282662868499756\n",
      "0.5072606801986694\n",
      "0.3820359706878662\n",
      "0.2461702674627304\n",
      "0.33621349930763245\n",
      "0.40120235085487366\n",
      "0.4782746732234955\n",
      "0.43749675154685974\n",
      "0.3744768500328064\n",
      "0.39862117171287537\n",
      "0.3332239091396332\n",
      "0.3228604793548584\n",
      "0.31690630316734314\n",
      "0.3951176404953003\n",
      "0.4088379144668579\n",
      "0.35586017370224\n",
      "0.40878814458847046\n",
      "0.3646528124809265\n",
      "0.2983950078487396\n",
      "0.5866393446922302\n",
      "0.272005170583725\n",
      "0.34865492582321167\n",
      "0.3520825505256653\n",
      "0.563169538974762\n",
      "0.3075799345970154\n",
      "0.33611470460891724\n",
      "0.3488198220729828\n",
      "0.29706650972366333\n",
      "0.4257115125656128\n",
      "0.30406638979911804\n",
      "0.2815062701702118\n",
      "0.3254089951515198\n",
      "0.32377922534942627\n",
      "0.35728719830513\n",
      "0.42680123448371887\n",
      "0.18755248188972473\n",
      "0.2428715080022812\n",
      "0.22126461565494537\n",
      "0.3444786071777344\n",
      "0.2718814015388489\n",
      "0.20533879101276398\n",
      "0.3980327248573303\n",
      "0.3374011218547821\n",
      "0.2907330095767975\n",
      "0.5212334990501404\n",
      "0.3363268971443176\n",
      "0.4184420108795166\n",
      "0.43741005659103394\n",
      "0.26006099581718445\n",
      "0.3848036825656891\n",
      "0.2433270514011383\n",
      "0.239938884973526\n",
      "0.4171993136405945\n",
      "0.2644926607608795\n",
      "0.2657795548439026\n",
      "0.49366867542266846\n",
      "0.49158817529678345\n",
      "0.3788209855556488\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "aggregated_losses = []\n",
    "for epoch in range(epochs):\n",
    "   model.train()\n",
    "   for x_batch, y_batch in train_loader:\n",
    "       y_pred=model(x_batch)\n",
    "       single_loss= loss_function(y_pred, y_batch)\n",
    "       aggregated_losses.append(single_loss.item())\n",
    "\n",
    "       optimizer.zero_grad()\n",
    "       single_loss.backward()\n",
    "       optimizer.step()\n",
    "\n",
    "       if (epoch+1)%25==0:\n",
    "           print(single_loss.item())\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ff970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
