{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a4bdff-f835-4f18-8953-fb6de7e0cbc7",
   "metadata": {},
   "source": [
    "### MNIST, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c5ccd-6ea5-4ece-a1c7-2f5630cc2c39",
   "metadata": {},
   "source": [
    "FashionMNIST, CIFAR10 데이터 받기 <br />\n",
    "LeNet5는 채널1인 것을 입력받는다.컬러의 경우, channel3일 경우에는 LeNet5의 입력채널을 3으로 변경해야한다. <br />\n",
    "__init__의 입력 채널 값이랑 nn.linear의 입력채널 값이 안 맞을 경우, 주로 에러가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec1785b-555b-4f0a-b709-def048fe4f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.v2 as v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67d2d247-3d34-4340-8c3b-84a43cc8e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.features = nn.Sequential(nn.Conv2d(1, 6, 5, padding='same'),\n",
    "                       nn.ReLU(),\n",
    "                       nn.MaxPool2d(2),\n",
    "                       nn.Conv2d(6, 16, 5, padding='same'),\n",
    "                       nn.ReLU(),\n",
    "                       nn.MaxPool2d(2),\n",
    "                       nn.Conv2d(16, 126, 5, padding='same'),\n",
    "                       nn.ReLU(),\n",
    "                       nn.MaxPool2d(2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Sequential(nn.Linear(126*3*3, 128),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(128, 64),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(64, 10),\n",
    "                                        nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #print(x.shape)\n",
    "        x =self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ebcae29-fc23-44d2-ad49-e86dfe436967",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LeNet5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30edd063-ee37-432a-999d-98dfee00a745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\torch-book\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#torchvision.datasets.MNIST(root: Union[str, Path], train: bool = True, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False)\n",
    "\n",
    "\n",
    "dataset =MNIST('data', download=False,transform=v2.ToTensor())  #(torch.Tensor, torch.Size([60000, 28, 28]))\n",
    "\n",
    "dataset =FashionMNIST('data', download=False,transform=v2.ToTensor())   #(torch.Tensor, torch.Size([60000, 28, 28]))\n",
    "\n",
    "dataset1 =CIFAR10('data', download=False,transform=v2.ToTensor())   #(numpy.ndarray, (50000, 32, 32, 3))  컬러이미지이다. LeNet5()에서 input channel이 1이 아닌 3이어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b563c7-554b-40f0-88f2-371b45f60b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([60000, 28, 28]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset.data), dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d782cf20-ada7-4eaf-b026-4544841ffec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "#           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "#           pin_memory=False, drop_last=False, timeout=0,\n",
    "#           worker_init_fn=None, *, prefetch_factor=2,\n",
    "#           persistent_workers=False)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da7d3bba-d75a-448e-9b1c-9bb5325eb44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for문 반복 횟수\n",
    "60000/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80101317-9f54-42cc-9765-eedccdffa139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
    "#우리가 받은 데이터셋은 image이다. 그런데 배치는 텐서, 넘파이배열, 숫자, 딕셔너리, 리스트만 가능하다. => import torchvision.transforms.v2 as v2 와 transform=v2.ToTensor() 추가\n",
    "\n",
    "for X_train, y_label in data_loader:\n",
    "    print(X_train.shape, y_label.shape)\n",
    "    break #for문을 1번만 실행하도록 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf2cca1f-26f5-4a4f-9759-74c028a01fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn= nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59520c14-5fd3-46db-9e46-ceec078de630",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92848afc-b745-4ff4-896b-1df9dec70aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_train, y_label in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = loss_fn(outputs, y_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da64b750-f7a8-4c7f-b3cc-42877165da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =MNIST('data', download=False,transform=v2.ToTensor())  #(torch.Tensor, torch.Size([60000, 28, 28]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e48a898a-2cad-4ab3-99fa-bd9dfa4004d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data , label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c44f1632-bedc-4ceb-b16a-b36c0178c1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc8c5cf3-dd4a-4055-9a8b-a5d3d9950c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset:\n",
    "    def __init__(self):\n",
    "        self.data=list(range(100, 200))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f57e04c-7db4-49bb-a514-dbb9c549d6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 101, 199)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=MyDataset()\n",
    "dataset[0], dataset[1], dataset[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a25961-c11d-4dce-ad2b-3498532328d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
